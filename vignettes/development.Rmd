---
title: "Development Thoughts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Development Thoughts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

**As of `r format(Sys.time(), "%B %d, %Y")`, this is still being *iterated*.**

# Background

The development of this project was inspired by the a problem of organizing many different hypotheses while writing my thesis. I had several data sets and many, many models, and had trouble with how to store and recall them. I wanted an easier way to pull my thoughts together, creating a dynamic structure that would unfold along with the research project itself.

The structure, **which is still being iterated**, is about an easy-to-understand interface that allows the following major concepts:

1. Identifying core data that will be queried with specific hypothesis
1. Ability to handle grouping/strata of that data for subsets of analyses
1. Forming hypotheses of multiple outcomes and multiple predictors with an epidemiological angle (e.g. *exposures*, *outcomes*, *covariates*)
1. Running and updating tests as the data changes
1. Extracting or recalling models as the research project progresses

As minimally-experienced programmer, I used examples of current effective packages while designing the API.

| Source | Descriptionn |
| --- | --- |
| [R4DS](https://r4ds.had.co.nz/many-models.html) | This was the first time I had seen an elegant way of generating multiple models and working with list-columns, particulary the type that could become *tidy*. |
| [{modelr}](https://modelr.tidyverse.org/) | An example of a package that simplifies modeling in R |
| [{modelgrid}](https://github.com/smaakage85/modelgrid) | A framework for creating and managing multiple models, with a focus on the {caret} package |
| [{parsnip}](https://parsnip.tidymodels.org/) | The core of this was based on the single interface for modeling that `tidymodels` provides and serves as a foundation for flexible model definitions |
| [{stacks}](https://stacks.tidymodels.org/) | An influential concept of an API designed for binding together mutliple model definitions, however is meant for a specific formula and pulling together multiple models for blended predictions |
| [{workflowsets}](https://workflowsets.tidymodels.org/index.html) | This fits multiple models in a workflow to identify a potential "best" model, which is very flexible |
| [{easystats}](https://easystats.github.io/easystats/) | Forms a different "universe" in parallel to the `tidyverse` for interpreting results of models, with a focus on the presentation and exploration of statistical analysis |
| [{ggdag}](https://ggdag.malco.io/) | A *tidy* approach to creating directed acyclic graphs |

The features that I found missing in these was the ability to handle a variety of formulas that had specific pre-defined hypotheses, and the focus had been to *compare* or *combine* models, instead of considering the focus on causality. Essentially, I wanted a more epidemiological approach. The *easystats* archetype of R "monsters" was partially the idea for the working name of **{aims}**. I started writing this after reading [this blogpost](https://blog.simonpcouch.com/blog/dev-docs-p1/) on development of a package. 

# On Names

The project name has gone through many iterations. The reason why the name was important is that the correct *metaphor* helps the planning process, providing a schema for understanding. [Metaphors We Live By](https://en.wikipedia.org/wiki/Metaphors_We_Live_By) by Lakoff was fundamental in this understanding for me.

| Project Name | Reasoning |
| --- | --- |
| {marksman} | The first draft included the concept of "specific aims". This research concept is from grant writing, where the first page of a project contains the specific hypotheses that will be explored. Thus, one could *load*, *aim*, and *fire* and specific targets. The problem with this was it seemed to deterministic, as exploration was a crucial component. |
| {octomod} | The second draft took inspiration from the idea of an *octopus*, with 8 arms or legs all surrounded a central idea. This allowed for flexibility in that the many arms of a project were not necessarily hierarchical. However, theming a project after a cephalopod does make the function names less easy to remember. |
| {aims} | Another consideration was to go back to the original concept of specific aims. Each project would be initialized, different study arms would be added, and then the corresponding models could be built. This seems more user-friendly in terms of nosological considerations. | 
| {diagnose} | After iterating through the code, modeling causality and looking at clinically relevant variables was the more refined underlying objective. This allowed for simplification of the hypothesis generating method and opened the door to look at effect modification. | 
| {dagger} | Although "diagnosing" the relationships between terms was potentially interpretable, it seemed to pull away from the original idea of the multiple potential relationships. Returning to the drawing board, the concept of mapping out relationships, like a **directed acyclic graph**, was the most persistent concept. |

# Defining the API

## First Draft

I've currently defined the basic functions as below:

-   `octomod()` initializes the modeling structure
-   `core()` defines the core data of the `octomod` structure, allowing tests to be performed from a centralized data set
-   `arm()` creates individual arms to the `octomod`, which reference a specific family of hypotheses
-   `equip()` gives each of the arms a *tidied* output including model fit and parameters, which can subsequently be called

The benefits of this are I can flexibly change the core data, give specific formulas that can be expanded/varied, fit independently, and extracted in a list-column based format.

The problems are that "changing" the central dataset is dangerous, the structure doesn't hold together afterwards, and fitting the data may break older models.

## Second Draft

### Core Functions

As described, the name helps to determine the function's purpose. The core functions represent the major user-facing features. They also help to identify the underlying structure needed to store the data collected.

- `project()` initializes a structure that holds and can retain information about the individual project aims or study arms
- `set_data()` attaches a specific data frame to the `project` object
- `add_arm()` creates a hypothesis-driven series of tests around a formula, and allows for multiple tests to be described simultaneously (e.g. sequential addition of covariates)
- `build_models()` runs the tests, whether it is basic statistics or more complex inferential models, with the capacity to update only the newly added arms
- `collect_findings()` recalls information from the `project` structure in a flexible way

**Defining the Structure**

There should be a core function that establishes the underlying structure. Should the data be given at this time, or added in a separate function? Initialization should likely be a list structure due to its malleability.

```{r, eval=FALSE}
project()
```

**Data Sets**

It seems that it should be a specific, but necessary, step to add data. The data structure, hopefully **tidy**, would provide the parameters by column names. It may allow for creating more effective selection commands as well.

```{r, eval=FALSE}
set_data(
  data = NULL,
  ...
)
```

**Adding Hypotheses**

This is probably the most important feature. A function such as `add_arm()` could help to stack on different potential tests. 

The complex part of this is that each *arm* would allow for variations of a hypothesis formula. Multiple predictors, multiple outcomes. In an epidemiological sense, its important that certain predictors are maintained as *exposures*. Each hypothesis thus needs to allow flexibility for the complexity of a model formula.

- Unique name for each arm, to be "pulled" later on
- A global formula or plan to be specified, and potentially varied or recombined (e.g. different "patterns" of combinations based on the testing strategy). It would be helpful to label certain variables as fixed exposures in some way as well.
- The type of "test" that is being used. This should likely draw from a common system, such as `parsnip`, to have consistent model representations. The other aspect would be more straightforward testing, such as from the `stats` base package.
- Subgroups of the data should be allowed to be analyzed via this as well. This is such a common pattern that it should likely be defined as a specific parameter.

```{r, eval=FALSE}
add_arm(
  title = "first_example",
  description = NULL,
  plan = outcome ~ exposure + covariate,
  exposure = "exposure",
  pattern = "parallel",
  test = linear_reg() %>% set_engine("lm"),
  ...
)
```

**Running Tests**

The ability to then run tests would be the next logical step. This capacity should allow for identifying if tests have already been run or not, and only run newly added components. 

```{r, eval=FALSE}
build_models(
  run_all = FALSE,
  which_tests = NULL,
  ...
)
```

**Data Extraction**

The final structure of the project should be flexible in terms of extracting or pulling data. Its imagined as a list structure, and thus components could be called by the `$` operator. That depends on quality of names, and making sure they are well described. 

```{r, eval=FALSE}
collect_findings(
  which_tests = NULL,
  ...
)
```

This is an important and difficult process of deciding how the user should be able to visualize information and extract findings. This will end up including `summary()` and `print()` functions as well.

### Project Structure

The overall project structure requires a specific *class* that will contain a pattern of an internal structure. The `project` object should hold:

- a space to hold the data set
- a *tidy* table that holds each `arm` as a separate row, containing columns for the title, formula combinations, modifiers/options, description, and build/run status
- a corresponding *tidy* table that holds the results of each `arm` after it has been run

## Third Draft

I realized this approach was not effective in that it was overly complicated in having multiple names, data frames, and connections between them. I simplified the pipeline to focus on a specific S3 classes that could be used to generate hypotheses, and iterate through to help look at variable relationship.

### Core Functions

```{r, eval=FALSE}
study() %>%
	add_hypothesis(hypothesis) %>%
	build_study()
```

Essentially, this was a list based concept of adding hypotheses to a modified list/tibble structure that had important data stored as attributes.

## Fourth Draft

I decided that I was not really seeing the big picture. What I wanted was a way to store a set of multiple tests with the purpose of studying the relationship between two variables (an exposure and an outcome), and potentially looking at covariates. The *basic* concept was essentially a **directed acyclic graph**, with the relationships storing the hypotheses that were tested. The functionality was the same, but the API was different.

The major classes needed would be ...

- `hypothesis` object, that showed and/or carried the relationship between variables, likely a complex formula object
- `map` object that drew the relationships between the variables, and allowed for the storage of multiple tests
- `path` object that is extracted after the map has been drawn (if you will), should be able to be evaluated to help understand which variables may be confounders, etc

